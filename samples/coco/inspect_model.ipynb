{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask R-CNN - Inspect Trained Model\n",
    "\n",
    "Code and visualizations to test, debug, and evaluate the Mask R-CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Path to Shapes trained weights\n",
    "SHAPES_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_shapes.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run one of the code blocks\n",
    "\n",
    "# Shapes toy dataset\n",
    "# import shapes\n",
    "# config = shapes.ShapesConfig()\n",
    "\n",
    "# MS COCO Dataset\n",
    "import coco\n",
    "config = coco.CocoConfig()\n",
    "COCO_DIR = \"path to COCO dataset\"  # TODO: enter value here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE_SHAPES                [[256 256]\n",
      " [128 128]\n",
      " [ 64  64]\n",
      " [ 32  32]\n",
      " [ 16  16]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [ 0.1  0.1  0.2  0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.5\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.002\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [ 123.7  116.8  103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           coco\n",
      "NUM_CLASSES                    81\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              2\n",
      "RPN_BBOX_STD_DEV               [ 0.1  0.1  0.2  0.2]\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TRAIN_ROIS_PER_IMAGE           128\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Override the training configurations with a few\n",
    "# changes for inferencing.\n",
    "class InferenceConfig(config.__class__):\n",
    "    # Run detection on one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Device to load the neural network on.\n",
    "# Useful if you're training a model on the same \n",
    "# machine, in which case use CPU and leave the\n",
    "# GPU for training.\n",
    "DEVICE = \"/cpu:0\"  # /cpu:0 or /gpu:0\n",
    "\n",
    "# Inspect the model in training or inference modes\n",
    "# values: 'inference' or 'training'\n",
    "# TODO: code for 'training' test mode not ready yet\n",
    "TEST_MODE = \"inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.86s)\n",
      "creating index...\n",
      "index created!\n",
      "Images: 35185\n",
      "Classes: ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
     ]
    }
   ],
   "source": [
    "# Build validation dataset\n",
    "if config.NAME == 'shapes':\n",
    "    dataset = shapes.ShapesDataset()\n",
    "    dataset.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "elif config.NAME == \"coco\":\n",
    "    dataset = coco.CocoDataset()\n",
    "    dataset.load_coco(COCO_DIR, \"minival\")\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()\n",
    "\n",
    "print(\"Images: {}\\nClasses: {}\".format(len(dataset.image_ids), dataset.class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create model in inference mode\n",
    "with tf.device(DEVICE):\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\n",
    "                              config=config)\n",
    "\n",
    "# Set weights file path\n",
    "if config.NAME == \"shapes\":\n",
    "    weights_path = SHAPES_MODEL_PATH\n",
    "elif config.NAME == \"coco\":\n",
    "    weights_path = COCO_MODEL_PATH\n",
    "# Or, uncomment to load the last model you trained\n",
    "# weights_path = model.find_last()\n",
    "\n",
    "# Load weights\n",
    "print(\"Loading weights \", weights_path)\n",
    "model.load_weights(weights_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image ID: coco.392144 (34940) http://cocodataset.org/#explore?id=392144\n",
      "Processing 1 images\n",
      "image                    shape: (1024, 1024, 3)       min:    0.00000  max:  255.00000\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  151.10000\n",
      "image_metas              shape: (1, 89)               min:    0.00000  max: 1024.00000\n",
      "gt_class_id              shape: (10,)                 min:    1.00000  max:   40.00000\n",
      "gt_bbox                  shape: (10, 5)               min:    0.00000  max: 1024.00000\n",
      "gt_mask                  shape: (1024, 1024, 10)      min:    0.00000  max:    1.00000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6gAAAOoCAYAAADcQdkoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n93W